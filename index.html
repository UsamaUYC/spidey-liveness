<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Live Selfie Session</title>
  <style>
    body {
      background: #f9f9f9;
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      text-align: center;
    }
    #videoContainer {
      position: relative;
      width: 100%;
      max-width: 420px;
      margin: 0 auto;
    }
    video, canvas {
      width: 100%;
      border-radius: 12px;
    }
    canvas {
      position: absolute;
      top: 0;
      left: 0;
    }
    #status {
      margin-top: 12px;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <h2>ğŸ§‘â€ğŸ’» Selfie Session</h2>
  <p>Align your face inside the frame</p>
  <div id="videoContainer">
    <video id="video" autoplay muted playsinline></video>
    <canvas id="overlay"></canvas>
  </div>
  <div id="status">ğŸ” Detecting face...</div>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>

  <script>
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const ctx = overlay.getContext('2d');
    const statusDiv = document.getElementById('status');

    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      return new Promise(resolve => video.onloadedmetadata = resolve);
    }

    async function detectFace() {
      const model = await blazeface.load();
      overlay.width = video.videoWidth;
      overlay.height = video.videoHeight;

      async function frame() {
        const predictions = await model.estimateFaces(video, false);
        ctx.clearRect(0, 0, overlay.width, overlay.height);

        if (predictions.length > 0) {
          const face = predictions[0];
          const [x, y] = face.topLeft;
          const [x2, y2] = face.bottomRight;
          const width = x2 - x;
          const height = y2 - y;

          // Draw face box
          ctx.strokeStyle = "lime";
          ctx.lineWidth = 3;
          ctx.strokeRect(x, y, width, height);

          // Check if face is centered
          const centerX = x + width / 2;
          const centerY = y + height / 2;
          const tolX = overlay.width / 3;
          const tolY = overlay.height / 3;

          if (
            centerX > tolX && centerX < overlay.width - tolX &&
            centerY > tolY && centerY < overlay.height - tolY
          ) {
            statusDiv.textContent = "âœ… Face aligned, capturing...";
            captureSelfie();
            return;
          } else {
            statusDiv.textContent = "ğŸ“ Align your face in the frame";
          }
        } else {
          statusDiv.textContent = "ğŸ” No face detected";
        }

        requestAnimationFrame(frame);
      }

      frame();
    }

    function captureSelfie() {
      const canvas = document.createElement("canvas");
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      const ctx = canvas.getContext("2d");
      ctx.drawImage(video, 0, 0);

      const imageData = canvas.toDataURL("image/png");
      video.srcObject.getTracks().forEach(track => track.stop());
      document.body.innerHTML = `<h2>âœ… Selfie captured and submitted!</h2>`;
      // send imageData to server or PC extension...
      console.log("Captured selfie base64:", imageData);
    }

    setupCamera().then(detectFace);
  </script>
</body>
</html>
